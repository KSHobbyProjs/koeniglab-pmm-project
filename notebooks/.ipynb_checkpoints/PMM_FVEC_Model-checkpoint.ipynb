{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PMM Implementation: FVEC with Gaussian Potential\n",
        "\n",
        "## Goal: Train a PMM to find the ground state eigenvalues of the 3D Hamiltonian\n",
        "$$\n",
        "  H = \\frac{\\hat{\\mathbf{p}}_\\mathbf{r}^2}{2\\mu} - V_0 \\exp(-\\mathbf{r}^2/R^2)\n",
        "$$\n",
        "for various volumes. The Hamiltonian can be written\n",
        "$$\n",
        "  H = \\frac{1}{a^2} T + V^{a^2},\n",
        "$$\n",
        "for some lattice spacing $a$. So, we'll train $T$ and $V$ in our PMM.\n",
        "### Background Theory\n",
        "For a pair of particles in an attractive Gaussian potential $V(\\mathbf{r})=-V_0\\exp(-\\mathbf{r}^2/R^2)$ ($\\mathbf{r} = \\mathbf{r}_1 - \\mathbf{r}_2$), the Hamiltonian is\n",
        "$$\n",
        "  H = \\frac{\\hat{\\mathbf{p}}_{\\mathbf{r}_1}}{2m_1} + \\frac{\\hat{\\mathbf{p}}_{\\mathbf{r}_2}}{2m_2} + V(\\mathbf{r}).\n",
        "$$\n",
        "Separating out the CM motion by separating variables with $\\mathbf{R} = \\frac{1}{m_1+m_2}\\left( m_1\\mathbf{r}_1 + m_2\\mathbf{r}_2\\right)$ and $\\mathbf{r} = \\mathbf{r}_1 - \\mathbf{r}_2$, we find\n",
        "$$\n",
        "  H_r = \\frac{\\hat{\\mathbf{p}}_{\\mathbf{r}}}{2\\mu} + V(\\mathbf{r})\n",
        "$$\n",
        "where $\\mu = m_1m_2 / (m_1 + m_2)$ is the reduced mass. The CM motion is just that of a free particle. <br><br>\n",
        "\n",
        "We discretize over a box with side length $L$. We to discretize with $N^3$ total lattice points ($N$ for each direction), so our lattice spacing is $a = L/N$ and we assume periodic boundary conditions. Thus, the relative coordinate goes from $r \\in [-N/2, N/2)$ (if one particle moves over the edge of the boundary, the relative coordinate flips and points in the opposite direction). We choose to index via:\n",
        "$$\n",
        "  i = \\left(x + \\frac{N}{2}\\right) + N \\left(y + \\frac{N}{2}\\right) + N^2\\left(z + \\frac{N}{2}\\right).\n",
        "$$\n",
        "If we chose to index via $i = x + N y + N^2 z$, we'd have to fold onto $[-N/2,N/2]$ when calculating the distance. Because the kinetic energy operator cares only about neighboring points (i.e., $u_{i+1} - 2 u_i + u_{i-1}$), the kinetic energy operator looks the same regardless of how we index.\n"
      ],
      "metadata": {
        "id": "Jkh0Ux67Kp3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Construct Hamiltonian"
      ],
      "metadata": {
        "id": "jwX66jyKWVRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lIhqEZJKbr1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as ss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from jax import config\n",
        "config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grab relative positions from index i\n",
        "def get_relative_coods(i, N):\n",
        "  if np.any(i < 0) or np.any(i > N**3 - 1):\n",
        "    print(i)\n",
        "    raise ValueError(f\"i must be between 0 and {N**3 - 1}\")\n",
        "  x = (i % N) - N / 2\n",
        "  y = ((i % N**2) // N) - N / 2\n",
        "  z = (i // N**2) - N / 2\n",
        "  return x, y, z\n",
        "\n",
        "# calculate relative distance given index i\n",
        "def get_distance(i, N):\n",
        "  x, y, z = get_relative_coods(i, N)\n",
        "  return np.sqrt(x**2 + y**2 + z**2)"
      ],
      "metadata": {
        "id": "tWRoGMHsVHeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_H(N, L, V0=-4.0, R=2.0):\n",
        "  a = L / N\n",
        "  N_tot = N**3\n",
        "  indices = np.arange(N_tot, dtype=np.int32)\n",
        "\n",
        "  # construct T\n",
        "  data = -6 * np.ones(N_tot)\n",
        "  rows = indices\n",
        "  cols = indices\n",
        "\n",
        "  x, y, z = get_relative_coods(indices, N)\n",
        "  neighbors = [(1, 0, 0), (-1, 0, 0), (0, 1, 0), (0, -1, 0), (0, 0, 1), (0, 0, -1)]\n",
        "  for dx, dy, dz in neighbors:\n",
        "    nx = ((x + dx) + N // 2) % N - N // 2                                      # x-neighbor physical cood\n",
        "    ny = ((y + dy) + N // 2) % N - N // 2                                      # y-neighbor physical cood\n",
        "    nz = ((z + dz) + N // 2) % N - N // 2                                      # z-neighbor physical cood\n",
        "    neighbor_indices= (nx + N // 2) + N * (ny + N // 2) + N**2 * (nz + N // 2)  # index of neighbor\n",
        "    rows = np.concatenate([rows, indices])\n",
        "    cols = np.concatenate([cols, neighbor_indices])\n",
        "    data = np.concatenate([data, np.ones(N_tot)])\n",
        "\n",
        "  T = -1 / a**2 * ss.coo_matrix((data, (rows, cols)), shape=(N_tot, N_tot), dtype=np.complex128).tocsr()\n",
        "\n",
        "  # construct V\n",
        "  distances = get_distance(indices, N)\n",
        "  vs = V0 * np.exp(-(distances * a)**2 / R**2)\n",
        "  V = ss.diags(vs, format='csr', dtype=np.complex128)\n",
        "\n",
        "  H = T + V\n",
        "  return H"
      ],
      "metadata": {
        "id": "Fgki71KSQXZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the ground-state energy for many volumes\n",
        "N = 32\n",
        "Ls = np.linspace(5, 20, 50)\n",
        "eigenvalues = np.zeros(len(Ls), dtype=np.float64)\n",
        "for i, L in enumerate(Ls):\n",
        "  H = construct_H(N, L)\n",
        "  eigvals, _ = ss.linalg.eigsh(H, k=1, which='SA')\n",
        "  print(f'Finished calculating eigenvalue for {L}')\n",
        "  eigenvalues[i] = eigvals[0]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(Ls, eigenvalues, '-')\n",
        "ax.set_xlabel(\"Volume (L)\")\n",
        "ax.set_ylabel(\"Ground State Energy\")\n",
        "ax.set_title(\"Ground State Energy vs Volume\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vVsw9WQAAN95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the PMM\n",
        "Construct matrices as $M(L) = A + LB$ as a first guess. The paper discusses that, for any function $f$ (under suitable conditions), we can find an affine eigenvalue PMM that reproduces $f(c)$. We could apply PMMs with a specific reference to the form of the Hamiltonian, but we don't need to, a PMM of the form $M(L) = A + LB$ will likely suffice."
      ],
      "metadata": {
        "id": "_mXq6oIk3_gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct Hamiltonian M after adjusting A, B\n",
        "def M(A, B, g):\n",
        "  # force hermiticity\n",
        "  A = .5 * (A + A.conj().T)\n",
        "  B = .5 * (B + B.conj().T)\n",
        "\n",
        "  # if g is given as a scalar, just return A + gB\n",
        "  # if g is given as a jnp array, return [M(g1), ..., M(gN)]\n",
        "  if jnp.isscalar(g):\n",
        "    return A + g * B\n",
        "  else:\n",
        "    # reshape g array so that it has dimension (:, 1, 1) for broadcasting with A, B\n",
        "    return A + g[:, None, None] * B\n",
        "\n",
        "# get the k-lowest eigenvalues of M (or Ms if M is given as a batch of matrices)\n",
        "def get_eigenvalues(M, k):\n",
        "  # M might not be sparse during construction, so use jnp instead of ss\n",
        "  if M.ndim == 2:\n",
        "    eigvals = jnp.linalg.eigvalsh(M)[:k]\n",
        "  elif M.ndim == 3:\n",
        "    #  jnp eig will vectorize, so pick k lowest eigs for each M_i if given multiple Ms\n",
        "    eigvals = jnp.linalg.eigvalsh(M)[:, :k]\n",
        "  else:\n",
        "    raise ValueError(\"Input must be a 2D matrix or 3D batch of matrices\")\n",
        "  return eigvals\n",
        "\n",
        "# loss function\n",
        "# mean squared error of the predicted eigenvalues to the true eigenvalues\n",
        "def loss(A, B, gs, Es):\n",
        "  # if Es are given as a 1D array of scalars, reshape to 2D array:\n",
        "  # [E1, E2, E3] -> [[E1], [E2], [E3]]\n",
        "  if Es.ndim == 1:\n",
        "    Es = Es.reshape(-1, 1)\n",
        "  # number of lowest-eigenvalues to take is given by shape of Es\n",
        "  k = Es.shape[1]\n",
        "  Ms = M(A, B, gs)\n",
        "  eigs = get_eigenvalues(Ms, k)\n",
        "  loss = jnp.mean(jnp.abs(eigs - Es)**2)\n",
        "  # L2 penalty\n",
        "  penalty = 1e-4 * (jnp.mean(jnp.abs(A)**2) + jnp.mean(jnp.abs(B)**2))\n",
        "  loss = loss + penalty\n",
        "  return loss\n",
        "\n",
        "# define general Adam-update for complex parameters and real-loss functions\n",
        "def adam_update(parameter, vt, mt, t, grad, eta=1e-2, beta1=0.9, beta2=0.999, eps=1e-8, absmaxgrad=1e3):\n",
        "  # conjugate the gradient and cap it with absmaxgrad\n",
        "  gt = jnp.clip(grad.real, -absmaxgrad, absmaxgrad) - 1j * jnp.clip(grad.imag, -absmaxgrad, absmaxgrad)\n",
        "  # compute the moments (momentum and normalizing) step parameters\n",
        "  vt = beta1 * vt + (1 - beta1) * gt\n",
        "  mt = beta2 * mt + (1 - beta2) * jnp.abs(gt)**2\n",
        "\n",
        "  # bias correction\n",
        "  vt_hat = vt / (1 - beta1 ** (t + 1))\n",
        "  mt_hat = mt / (1 - beta2 ** (t + 1))\n",
        "\n",
        "  # step parameter\n",
        "  parameter = parameter - eta * vt_hat / (jnp.sqrt(mt_hat) + eps)\n",
        "  return parameter, vt, mt\n",
        "\n",
        "def train(A, B, epochs, gs, Es, store_loss=100):\n",
        "  # construct vt and mt moments\n",
        "  shape = A.shape\n",
        "  if shape != B.shape:\n",
        "    raise ValueError(\"A and B must have the same shape\")\n",
        "  vt_A = jnp.zeros(shape, dtype=jnp.complex128)\n",
        "  mt_A = jnp.zeros(shape, dtype=jnp.complex128)\n",
        "\n",
        "  vt_B = jnp.zeros(shape, dtype=jnp.complex128)\n",
        "  mt_B = jnp.zeros(shape, dtype=jnp.complex128)\n",
        "\n",
        "  # create array to store loss at epoch t\n",
        "  losses = np.zeros(epochs // store_loss)\n",
        "\n",
        "  # jit the loss function so that it's significantly quicker to call\n",
        "  jit_loss = jax.jit(loss)\n",
        "\n",
        "  # define the gradient function\n",
        "  grad_loss = jax.grad(jit_loss, argnums=(0, 1))\n",
        "\n",
        "  # update the parameter for every epoch\n",
        "  for t in range(epochs):\n",
        "    gt = grad_loss(A, B, gs, Es)\n",
        "    A, vt_A, mt_A = adam_update(A, vt_A, mt_A, t, gt[0])\n",
        "    B, vt_B, mt_B = adam_update(B, vt_B, mt_B, t, gt[1])\n",
        "\n",
        "    if t % store_loss == 0:\n",
        "      losses_at_t = jit_loss(A, B, gs, Es)\n",
        "      losses[t // store_loss] = losses_at_t\n",
        "\n",
        "  # force hermiticity one last time\n",
        "  A = .5 * (A + A.conj().T)\n",
        "  B = .5 * (B + B.conj().T)\n",
        "\n",
        "  return A, B, losses"
      ],
      "metadata": {
        "id": "vCq8XIAs2F7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up training data for PMM\n",
        "k_num = 1\n",
        "Ls_train = np.linspace(5, 20, 20)\n",
        "eigenvalues_train = np.zeros((len(Ls_train), k_num), dtype=np.float64)\n",
        "for i, L in enumerate(Ls_train):\n",
        "  H = construct_H(N, L)\n",
        "  eigvals, _ = ss.linalg.eigsh(H, k=k_num, which='SA')\n",
        "  eigvals = np.sort(eigvals)\n",
        "  print(f'Finished calculating eigenvalue for {L}:', eigvals)\n",
        "  eigenvalues_train[i] = eigvals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "sfLQa4tE-Uw-",
        "outputId": "61879427-96a4-4ea8-d712-7e9479bced95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1793098753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set up training data for PMM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mk_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mLs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0meigenvalues_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15000\n",
        "n = 3\n",
        "store_loss = 100\n",
        "\n",
        "mag = .5e-1\n",
        "seed = 1 # 1 (mag = .5e-1, everything else default)\n",
        "key = jax.random.key(seed)\n",
        "key1, key2 = jax.random.split(key, 2)\n",
        "A = jax.random.normal(key1, shape=(n, n), dtype=jnp.complex128)\n",
        "B = jax.random.normal(key2, shape=(n, n), dtype=jnp.complex128)\n",
        "\n",
        "A = .5 * (A + A.conj().T)\n",
        "B = .5 * (B + B.conj().T)\n",
        "\n",
        "A *= mag\n",
        "B *= mag\n",
        "\n",
        "# train the PMM using a sample set of L and eigenvalues defined above\n",
        "Ls_train = jnp.array(Ls_train)\n",
        "eigenvalues_train = jnp.array(eigenvalues_train)\n",
        "A, B, losses = train(A, B, epochs, Ls_train, eigenvalues_train, store_loss=store_loss)\n",
        "print(f\"Loss at epoch {(len(losses) - 1)* store_loss}: {losses[-1]}\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(store_loss * np.arange(len(losses)), np.log10(losses), '-')\n",
        "ax.set_ylabel(r'$L$')\n",
        "ax.set_xlabel(r'$t$')\n",
        "ax.set_title(r'Losses vs Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "so6rb7tK8lQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ls = jnp.array(Ls)\n",
        "eigenvalues_model = get_eigenvalues(M(A, B, Ls), k_num)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(Ls, eigenvalues, '-', label='Exact Values')\n",
        "ax.plot(Ls, eigenvalues_model[:,0], '--', label='Model Values')\n",
        "ax.plot(Ls_train, eigenvalues_train[:,0], 'o', label='Training Values')\n",
        "ax.set_xlabel(r'$L$')\n",
        "ax.set_ylabel(r'$E_0$')\n",
        "ax.set_title(r'Ground-state energy as a function of $L$')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "percent_error = 100 * np.abs(get_eigenvalues(M(A, B, Ls_train), k_num) - eigenvalues_train) / np.abs(eigenvalues_train)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(Ls_train, percent_error, '-')\n",
        "ax.set_xlabel('L')\n",
        "ax.set_ylabel('% Error')\n",
        "ax.set_title('% Error b/w Model-predicted Eigenvalues to Training Eigenvalues vs Volume')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "XVtZeBRVAO2B",
        "outputId": "7750517a-3914-43b1-910a-9366dae3eb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'jnp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1565558740.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meigenvalues_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_eigenvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigenvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Exact Values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'jnp' is not defined"
          ]
        }
      ]
    }
  ]
}